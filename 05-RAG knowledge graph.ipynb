{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG - Graphe de connaissance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation du graphe avec ses données\n",
    "\n",
    "Le KG est un exemple simplifié des objets et relations typique d'un outil de SDLC/Devops (team, tasks, service, etc.)\n",
    "\n",
    "<img src=\"./assets/kag.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '.venv (Python 3.11.7)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "import json\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "\n",
    "with open('./docs/microservices.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(f\"data: \\n {data}\")\n",
    "    graph.query(data['query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation du graphe\n",
    "\n",
    "Nous utilisons `langchain` pour peupler créer un vector store sur la propriété `description`des noeuds `Task`du graphe\n",
    "\n",
    "[`Neo4j`](http://localhost:7474/browser/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(),\n",
    "    index_name='tasks',\n",
    "    node_label=\"Task\",\n",
    "    text_node_properties=['name', 'description', 'status'],\n",
    "    embedding_node_property='embedding',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recherche de similarité et chat\n",
    "\n",
    "Le vector store est maintenant disponible pour de la recherche de similarité sur la description des Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = vector_index.similarity_search(\n",
    "    \"How will RecommendationService be updated?\"\n",
    ")\n",
    "print(response[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la même manière nous pouvons maintenant utiliser notre vecteur store dans le KG pour alimenter le LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "vector_qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(), chain_type=\"stuff\", retriever=vector_index.as_retriever())\n",
    "\n",
    "vector_qa.invoke(\n",
    "    {\"query\": \"How will recommendation service be updated?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Les limites du RAG simple (naïf?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_qa.invoke(\n",
    "    {\"query\": \"Combien y-a-t-il de ticket Open?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le LLM est affirmatif mais la réponse est incorrecte !\n",
    "\n",
    "Ce n'est pas une hallucination du LLM mais une limite de notre RAG.\n",
    "En effet la qualité de la réponse du LLM est directement corrélé à la qualité et quantité des informations pertinentes trouvées dans le vector store. Ici notre chaîne est paramétrée par défaut pour envoyer au LLM les 4 documents les plus pertinents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utiliser le LLM pour produire directement une requête sur le KG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain fournit une chaîne préconfigurée avec un agent disposant d'un Tool lui permettant de générer et exécuter des requêtes sur le Knowledge Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "graph.refresh_schema()\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm = ChatOpenAI(temperature=0, model_name='gpt-4'),\n",
    "    qa_llm = ChatOpenAI(temperature=0), graph=graph, verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_chain.invoke(\n",
    "    {\"query\": \"Combien y-a-t-il de ticket ouverts??\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cypher_chain.invoke(\n",
    "    {\"query\": \"Which services depend on Database indirectly?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixer RAG et Knowledge graph dans un Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_functions_agent, Tool, AgentExecutor\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Tasks\",\n",
    "        func=vector_qa.invoke,\n",
    "        description=\"\"\"Useful when you need to answer questions about descriptions of tasks.\n",
    "        Not useful for counting the number of tasks.\n",
    "        Use full question as input.\n",
    "        \"\"\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Graph\",\n",
    "        func=cypher_chain.invoke,\n",
    "        description=\"\"\"Useful when you need to answer questions about microservices,\n",
    "        their dependencies or assigned people. Also useful for any sort of\n",
    "        aggregation like counting the number of tasks, etc.\n",
    "        Use full question as input.\n",
    "        \"\"\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompt = prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful assistant\"),\n",
    "  (\"placeholder\", \"{chat_history}\"),\n",
    "  (\"human\", \"{input}\"),\n",
    "  (\"placeholder\", \"{agent_scratchpad}\")]\n",
    ")\n",
    "agent = create_openai_functions_agent(\n",
    "    ChatOpenAI(temperature=0, model_name='gpt-4'), tools, prompt\n",
    ")\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke({\"input\": \"Which team is assigned to maintain PaymentService?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke({\"input\": \"Which tasks have optimization in their description?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_in_actions_notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
