{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat bot simple avec mémoire\n",
    "\n",
    "<img src=\"../assets/simple_chatbot_with_memory.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables d'environnement\n",
    "Charger les variables d'environnement (clé OpenAI, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation du Chat Model\n",
    "Le client du LLM est instancié. La classe `ChatOpenAI` implémente la classe de base `BaseChatModel` partagée avec `AzureChatOpenAi`, `ChatMistralAI`, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation du Prompt Template\n",
    "\n",
    "Le `prompt template` contient 3 parties:\n",
    "* Un message `System` contenant les instructions pour le LLM (rôle, formattage, etc.)\n",
    "* Un `Placeholder` dans lequel sera injecté le contenu de la mémoire\n",
    "* Un message `Human` dans lequel la question utilisateur sera injectée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['history', 'question'] input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful AI bot. Respond in french.')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,  \n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate, \n",
    "    MessagesPlaceholder\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "       SystemMessagePromptTemplate.from_template( \"You are a helpful AI bot. Respond in french.\"),\n",
    "       MessagesPlaceholder(variable_name=\"history\"),\n",
    "       HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la chaîne finale\n",
    "La chaîne est une simple séquence de deux `Runnables`:\n",
    "* Le *prompt template*, dans lequel sera injectée les *inputs*\n",
    "* Le *model* prenant en entrée le *prompt* variabilisé et dont l'output sera la *réponse finale*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invocation de la chaîne\n",
    "La chaîne est invoquée en lui fournissant les inputs nécessaires:\n",
    "* La question utilisateur\n",
    "* La liste des messages précédents issue de l'objet mémoire, pour le moment un tableau vide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse:\n",
      "  Bonjour Laurent ! Comment puis-je vous aider aujourd'hui ?\n"
     ]
    }
   ],
   "source": [
    "first_question = \"Hi ! My name is Laurent\"\n",
    "\n",
    "first_response = chain.invoke(\n",
    "    {\n",
    "        \"question\": first_question,\n",
    "        \"history\": []\n",
    "    }\n",
    ")\n",
    "print(f\"Réponse:\\n  {first_response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gestion de la mémoire\n",
    "\n",
    "Les LLM étant stateless, sans gestion de la mémoire de notre part toute question suivante posée sera décorellée de la précédente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Je suis désolé, mais je ne peux pas connaître ton nom. Comment t'appelles-tu ?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_question=\"What is my name ?\"\n",
    "\n",
    "response_without_memory = chain.invoke(\n",
    "    {\n",
    "        \"question\": second_question,\n",
    "        \"history\": []\n",
    "    }\n",
    ")\n",
    "response_without_memory.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici pour gérer la mémoire nous utiliserons la classe `ChatMessageHistory` qui va stocker la mémoire dans un simple array. Pour une vrai utilisation, on préférera évidemment persister la mémoire. Pour cela Langchain propose de nombreuses intégrations: `SQL`, `Postgres`, `Mongo`, `ElasticSearch`, `Redis`,... (qui implémentent toutes la même interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "chat_history.add_user_message(first_question)\n",
    "chat_history.add_ai_message(first_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de la nouvelle invocation, il nous suffit de passer en input la liste des messages de la mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Votre nom est Laurent.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_with_memory = chain.invoke(\n",
    "    {\n",
    "        \"question\": \"What is my name ?\",\n",
    "        \"history\": chat_history.messages\n",
    "    }\n",
    ")\n",
    "response_with_memory.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
