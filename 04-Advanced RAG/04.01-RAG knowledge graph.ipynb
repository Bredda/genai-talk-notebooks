{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG - Graphe de connaissance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation du graphe avec ses données\n",
    "\n",
    "Graphe de connaissace: exemple simplifié des objets et relations typique d'un outil de SDLC/Devops (team, tasks, service, etc.)\n",
    "\n",
    "<img src=\"../assets/kag.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du grpahe Neo4J\n",
    "\n",
    "Un graphe de connaissance est construit à partir d'un fichier Json. Le grpahe comprend une trentaine de noeuds représentant équipe, microservice, tasks et personnes, visible sur le  [dashboard Neo4j](http://localhost:7474/browser/) (`MATCH (n) RETURN n`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "import json\n",
    "\n",
    "from os import getenv\n",
    "\n",
    "graph = Neo4jGraph(username=getenv(\"NEO4J_USERNAME\"), password=getenv(\"NEO4J_PASSWORD\"))\n",
    "\n",
    "with open('../__docs__/microservices.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(f\"data: \\n {data}\")\n",
    "    graph.query(data['query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation du graphe\n",
    "\n",
    "Nous utilisons `langchain` pour peupler créer un vector store sur la propriété `description`des noeuds `Task`du graphe.\n",
    "\n",
    "<img src=\"../assets/neo4j_graph2.png\" alt=\"drawing\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(),\n",
    "    index_name='tasks',\n",
    "    node_label=\"Task\",\n",
    "    text_node_properties=['name', 'description', 'status'],\n",
    "    embedding_node_property='embedding',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recherche de similarité et chat\n",
    "\n",
    "Le vector store est maintenant disponible pour de la recherche de similarité sur la description des Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = vector_index.similarity_search(\n",
    "    \"How will RecommendationService be updated?\"\n",
    ")\n",
    "print(response[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le vecteur store associé au champ `description` des Tasks du graphe est donc accessible au modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"Je dispose de données provenant d'un outil de gestion du cycle de vie logiciel (SDLC) qui incluent les informations suivantes :\n",
    "\n",
    "Équipes de développement : noms des équipes, projets sur lesquels elles travaillent, compétences clés.\n",
    "Microservices : noms des microservices, technologies utilisées, propriétaires, statut (en développement, en production, en maintenance).\n",
    "Tasks : tâches assignées, priorités, deadlines, état d'avancement, équipes et personnes assignées.\n",
    "Personnes : noms, rôles, compétences, historique des tâches réalisées.\n",
    "J'ai besoin de synthétiser et d'obtenir des réponses précises basées sur ces données. Par exemple :\n",
    "\n",
    "Quels sont les microservices gérés par l'équipe X ?\n",
    "Qui est responsable de la tâche Y et quelle est sa deadline ?\n",
    "Quelles compétences sont présentes au sein de l'équipe Z ?\n",
    "Utilise les données que tu récupères pour fournir des réponses contextuelles et pertinentes aux questions liées à ces entités (équipes, microservices, tâches, personnes).\"\"\"),\n",
    "        (\"system\", \"{context}\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "prompt_template\n",
    "chain = prompt_template | ChatOpenAI() | vector_index.as_retriever()\n",
    "\n",
    "vector_qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(), chain_type=\"stuff\", retriever=vector_index.as_retriever())\n",
    "\n",
    "vector_qa.invoke(\n",
    "    {\"query\": \"Comment sera mis-à-jour le service de recommandation ?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Les limites du RAG simple (naïf?)\n",
    "\n",
    "Dans le cas de données très structurées comme ici, le RAG classique (\"naïf\") montre vite ses limites.\n",
    "\n",
    "Le principe du RAG est de récupérer (*retrieve*) les `k` tops documents (*chunk* de nos documents originaux) du vector store possédant le plus de similarité avec notre question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_qa.invoke(\n",
    "    {\"query\": \"Combien y-a-t-il de ticket Open?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le LLM est affirmatif mais la réponse est incorrecte !\n",
    "\n",
    "Ce n'est pas une hallucination du LLM mais une limite de notre RAG.\n",
    "En effet la qualité de la réponse du LLM est directement corrélé à la qualité et quantité des informations pertinentes trouvées dans le vector store. Ici notre chaîne est paramétrée par défaut pour envoyer au LLM les 4 documents les plus pertinents.\n",
    "\n",
    "Grâce à la requête `MATCH (t:Task) WHERE t.status = 'open' RETURN t` nous voyons que le graphe contient 5 tickets open et non pas 3. \n",
    "\n",
    "<img src=\"../assets/neo4j_graph3.png\" alt=\"drawing\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utiliser le LLM pour produire directement une requête sur le KG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain fournit une chaîne préconfigurée `GraphCypherQAChain` avec un agent disposant d'un Tool lui permettant de générer et exécuter des requêtes sur le Knowledge Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "graph.refresh_schema()\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm = ChatOpenAI(temperature=0, \n",
    "    model_name='gpt-4o'),\n",
    "    qa_llm = ChatOpenAI(temperature=0), \n",
    "    graph=graph, \n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_chain.invoke(\n",
    "    {\"query\": \"Combien y-a-t-il de ticket ouverts??\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant pleinement tirer parti de la nature structurée de nos data, par exemple en posant des questions sur des relations indirectes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cypher_chain.invoke(\n",
    "    {\"query\": \"Which services depend on Database indirectly?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixer RAG et Knowledge graph dans un Agent\n",
    "\n",
    "Au final afin de tirer les bénéfices des 2 cas d'usage, nous pouvons construire un agent possédant ces 2 outils:\n",
    "* Rag naïf - en cas de besoin de la description des tasks\n",
    "* Requête sur le graphe - en cas de question sur les relations ou le dénombrage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_functions_agent, Tool, AgentExecutor\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Tasks\",\n",
    "        func=vector_qa.invoke,\n",
    "        description=\"\"\"Useful when you need to answer questions about descriptions of tasks.\n",
    "        Not useful for counting the number of tasks.\n",
    "        Use full question as input.\n",
    "        \"\"\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Graph\",\n",
    "        func=cypher_chain.invoke,\n",
    "        description=\"\"\"Useful when you need to answer questions about microservices,\n",
    "        their dependencies or assigned people. Also useful for any sort of\n",
    "        aggregation like counting the number of tasks, etc.\n",
    "        Use full question as input.\n",
    "        \"\"\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompt = prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful assistant\"),\n",
    "  (\"placeholder\", \"{chat_history}\"),\n",
    "  (\"human\", \"{input}\"),\n",
    "  (\"placeholder\", \"{agent_scratchpad}\")]\n",
    ")\n",
    "agent = create_openai_functions_agent(\n",
    "    ChatOpenAI(temperature=0, model_name='gpt-4o'), tools, prompt\n",
    ")\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke({\"input\": \"Which team is assigned to maintain PaymentService?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke({\"input\": \"Which tasks have optimization in their description?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_in_actions_notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
