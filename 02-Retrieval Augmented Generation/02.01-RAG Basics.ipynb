{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RETRIEVAL AUGMENTED GENERATION\n",
    "\n",
    "<img src=\"../assets/rag_macro.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation du model\n",
    "Nous réutilisons le même modèle que pour le chat bot simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation du Prompt Template\n",
    "\n",
    "Nous procédons à deux évolutions dans le `prompt template`:\n",
    "* Le message System évolue pour préciser ce qui est attendu du modèle\n",
    "* Un nouveau placeholder est créé pour y injecter les documents pertinents à fournir au modèle pour générer la réponse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are an assistant for question-answering tasks. \n",
    "         Use the following pieces of retrieved context to answer the question. \n",
    "         If you don't know the answer, just say that you don't know. \n",
    "         Use three sentences maximum and keep the answer concise.\"\"\"),\n",
    "        (\"placeholder\", \"{history}\"),\n",
    "        (\"system\", \"{context}\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation de la mémoire\n",
    "Pour la mémoire nous utiliserons cette fois-ci une instance Redis locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "history = RedisChatMessageHistory(\"genai_in_action_simple_rag\", url=getenv(\"REDIS_URL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingestion du document\n",
    "\n",
    "<img src=\"../assets/rag_load_split_embed.png\" alt=\"drawing\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'étape `Load`, nous utilisons un des nombreux `Loader` natif de Langchain, ici celui traitant les fichiers `txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# LOAD\n",
    "loader = PyPDFLoader(\"../__docs__/NO-GRH-010 Gestion des déplacements et défraiement_1.0.pdf\")\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:250])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le `Split`, le document est découpé en *chunk* de 1000 caractères avec un recoupement (*overlap*) entre eux de 200 caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# SPLIT\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin pour la partie `Embed`et `Store`, nous faisons appel au modèle de LLM d'embedding d'penAi (`ada`) pour réaliser les embeddings et les stockons dans le vector database du [Redis local](http://localhost:8001/redis-stack/browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.redis import Redis\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "vectorstore = Redis.from_documents(\n",
    "    splits,\n",
    "    OpenAIEmbeddings(),\n",
    "    redis_url=getenv(\"REDIS_URL\"),\n",
    "    index_name=\"genai_in_action_simple_rag\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la chaîne OpenAI\n",
    "\n",
    "La chaîne finale se complexifie pour être une composition de plusieurs chaînes:\n",
    "\n",
    "La chaîne principale est une chaîne séquentielle qui va :\n",
    "1. Traiter les inputs\n",
    "2. Passer ces inputs traités au prompt template\n",
    "3. Passer le prompt termplate variabilisé au model\n",
    "4. Passer la réponse à un parser pour en extraire simplement la réponse au format texte\n",
    "\n",
    "Le premier élément de la chaîne est elle-même une chaîne qui va traiter les inputs en parrallèle:\n",
    "* `question` et `history` sont simplement passé tels quels\n",
    "* `context` est produit via une 3ème chaîne séquentielle qui va à partir de la question de l'utilistaeur retourné les bouts de document les plus pertinents du vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    # Runnable parallèles\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | vectorstore.as_retriever() | format_docs, \n",
    "        \"question\": itemgetter(\"question\"), \n",
    "        \"history\": itemgetter(\"history\")\n",
    "    }\n",
    "    # Runnable séquentiels\n",
    "    | prompt_template\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant poser des questions sur le document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 =\"Quelle est la politique de déplacement de Younup ?\"\n",
    "\n",
    "response1 = rag_chain.invoke(\n",
    "    {\n",
    "        \"question\": question1,\n",
    "        \"history\": history.messages\n",
    "    }\n",
    ")\n",
    "response1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: Streamer la réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 =\"Quelles sont les personnes à contacter ?\"\n",
    "response2 = \"\"\n",
    "\n",
    "for chunk in rag_chain.stream(\n",
    "    {\n",
    "        \"question\": question2,\n",
    "        \"history\": history.messages\n",
    "    }):\n",
    "    print(chunk)\n",
    "    response2+=chunk\n",
    "\n",
    "print(f\"Réponse complète: {response2}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
