{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du State de notre graph\n",
    "\n",
    "Dans cette première étape, nous définissons la structure de données qui servira à stocker les informations à chaque étape de la recherche et du traitement des réponses dans le graphe. Cette structure, `ComplexityState`, est une classe dérivée de `TypedDict`, qui offre une manière typée de gérer les données essentielles dans le contexte d'un flux de réponses IA. \n",
    "\n",
    "Les attributs principaux incluent :\n",
    "\n",
    "- `title` : Titre de la requête ou du sujet de recherche.\n",
    "- `messages` : Historique des messages ou des échanges, composés d'une séquence de messages (`BaseMessage`).\n",
    "- `search_queries` et `related_queries` : Liste des requêtes de recherche initiales et des requêtes associées générées.\n",
    "- `search_results`, `images`, et `videos` : Résultats de recherche, liens d'images et de vidéos pertinents.\n",
    "- `scrapped_results` : Données extraites de pages web.\n",
    "- `page_summaries` : Résumés des pages pertinentes, ajoutés pour une analyse rapide.\n",
    "\n",
    "Cette classe `ComplexityState` fournit une base de données centralisée qui permet à chaque étape du flux de travail de lire, écrire ou mettre à jour les informations collectées, assurant ainsi une cohérence dans l'état du graphe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ComplexitySubQueries(BaseModel):\n",
    "    \"\"\"List of internet search queries\"\"\"\n",
    "    queries: list[str] = Field(description=\"List of the generated internet search queries\")\n",
    "    title: str = Field(description=\"Title for the user chat\")\n",
    "\n",
    "\n",
    "class ComplexityRelatedQueries(BaseModel):\n",
    "    \"\"\"List of internet search queries\"\"\"\n",
    "    queries: list[str] = Field(description=\"List of the generated related search queries\")\n",
    "\n",
    "\n",
    "class ComplexityState(TypedDict):\n",
    "    title: str\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    search_queries: list[str]\n",
    "    related_queries: list[str]\n",
    "    search_results: list[str]\n",
    "    images: list[str]\n",
    "    videos: list[str]\n",
    "    scrapped_results: list[str]\n",
    "    page_summaries: Annotated[list, operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions support\n",
    "\n",
    "Dans cette section, nous définissons plusieurs fonctions support pour faciliter l'implémentation et la lisibilité du graphe. Ces fonctions permettent d'effectuer des recherches web, d'accéder à des images et des vidéos pertinentes via la bibliothèque duckduckgo, et de configurer un modèle de langage pour gérer les interactions entre les différents nœuds.\n",
    "\n",
    "Ces fonctions offrent une structure de base pour gérer les actions de recherche et d'interaction du graphe, posant les fondations de la chaîne de traitement qui sera exploitée dans les étapes suivantes.\n",
    "\n",
    "#### Recherche Web\n",
    "\n",
    "\n",
    "Ces trois fonctions utilisent duckduckgo pour effectuer des recherches web, d'images, et de vidéos, respectivement. Elles simplifient l'accès aux informations et ressources multimédias en lien avec les requêtes de l'utilisateur.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import AsyncDDGS\n",
    "\n",
    "\n",
    "async def _search_web(query):\n",
    "    return await AsyncDDGS(proxy=None).atext(query, max_results=2)\n",
    "\n",
    "async def _search_videos(query):\n",
    "    return await AsyncDDGS(proxy=None).avideos(query, max_results=2)\n",
    "\n",
    "async def _search_images(query):\n",
    "    return await AsyncDDGS(proxy=None).aimages(query, max_results=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création d'un model \"générique\"\n",
    "\n",
    "Cette fonction instancie un modèle ChatOpenAI destiné à être utilisé par les nœuds du graphe. \n",
    "\n",
    "Pour cette démonstration, nous utilisons un modèle unique pour simplifier l'implémentation et la lisibilité. Cependant, il est possible d'utiliser divers modèles et même différents fournisseurs en fonction des exigences spécifiques de chaque nœud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "        model='gpt-4o-mini',\n",
    "        temperature=0,\n",
    "        api_key=getenv(\"OPENAI_API_KEY\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observability\n",
    "\n",
    "Enfin, nous incluons une fonction qui crée un simple callback LangChain pour Langfuse, permettant de \"déverser\" les évènements du graphe dans [Langfuse](http://localhost:3003) pour apporter de l'observabilité à notre graphe.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "def langfuse_handler(session_id: str):\n",
    "    return CallbackHandler(\n",
    "    secret_key=getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    public_key=getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    host=getenv(\"LANGFUSE_HOST\"),\n",
    "    session_id=session_id,\n",
    "    user_id=\"user-id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des différents noeuds\n",
    "\n",
    "Dans cette section, nous définissons les fonctions de création des nœuds du graphe. Ces nœuds sont les éléments de base de notre flux de traitement et utilisent les fonctions support pour effectuer des recherches, extraire des données et générer des réponses. Chaque nœud exécute une étape spécifique, et ensemble, ils constituent le flux complet de génération de réponse.\n",
    "\n",
    "Ces nœuds forment la logique principale de traitement du graphe, orchestrant la recherche, l’analyse et la génération de réponse finale. Ensemble, ils permettent de transformer une requête utilisateur en une réponse complète, enrichie et multimodale.\n",
    "\n",
    "#### Générateur de requête\n",
    "\n",
    "Appelle un modèle de langage pour générer trois requêtes web à partir de la requête utilisateur initiale. Ces requêtes élargissent le contexte de recherche et permettent de couvrir des angles divers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "genereate_queries_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"\"\"Provide a list of 3 better search queries for \n",
    "            web search engine to answer the given question.\n",
    "            Answer:\"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "async def genereate_queries(state: ComplexityState):\n",
    "    structured_model = model.with_structured_output(ComplexitySubQueries)\n",
    "    chain = genereate_queries_template | structured_model \n",
    "    messages = state['messages']\n",
    "    result = await chain.ainvoke(messages)\n",
    "    return {\"search_queries\":result.queries, \"title\":result.title}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exécuteur de requêtes Internet\n",
    "\n",
    "`search_web` utilise la fonction support de recherche web pour exécuter chaque requête générée par generate_queries. Grâce à asyncio, ces recherches sont parallélisées pour optimiser les performances.\n",
    "\n",
    "`search_images` et `search_videos` appellent respectivement les fonctions de recherche d'images et de vidéos pour chacune des trois requêtes générées par generate_queries. Comme pour search_web, ces recherches sont parallélisées avec asyncio pour améliorer les performances et maximiser la diversité des résultats visuels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def search_web(state: ComplexityState):\n",
    "    queries = state['search_queries']\n",
    "    tasks = [_search_web(q) for q in queries]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return {\"search_results\":results}\n",
    "\n",
    "async def search_videos(state: ComplexityState):\n",
    "    queries = state['search_queries']\n",
    "    tasks = [_search_videos(q) for q in queries]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    flatten_results = [item for sublist in results for item in sublist]\n",
    "    return {\"videos\":flatten_results}\n",
    "\n",
    "async def search_images( state: ComplexityState):\n",
    "    queries = state['search_queries']\n",
    "    tasks = [_search_images(q) for q in queries]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    flatten_results = [item for sublist in results for item in sublist]\n",
    "    return {\"images\":flatten_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrap Web\n",
    "\n",
    "Utilise un `Loader` fourni par `LangChain` pour récupérer et structurer le contenu des pages web trouvées lors de la recherche précédente. Cela permet d'extraire du texte des pages sélectionnées pour une analyse plus approfondie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "\n",
    "async def scrap_web(state: ComplexityState):\n",
    "    search_results = state['search_results']\n",
    "    urls = [result['href'] for search_result in search_results for result in search_result]\n",
    "    loader = AsyncHtmlLoader(urls)\n",
    "    docs = loader.load()\n",
    "    html2text = Html2TextTransformer()\n",
    "    docs_transformed = html2text.transform_documents(docs)\n",
    "    return {\"scrapped_results\":docs_transformed}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sumarize Web\n",
    "\n",
    " Appelle un modèle de langage pour générer un résumé de chaque page web scrappée, condensant l'information et facilitant son utilisation dans les étapes suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"\"\"You are an expert at summarizing web search result pages.\n",
    "            Your task is to summarize the following content in order for anoter assitant to respond to user question. \n",
    "            Do mention the source in your summay.\n",
    "            Do NOT answer directly to the question, another expert will take care of this task.\n",
    "            Question: {query}\n",
    "            Source: {source}\n",
    "            Content: {content} \"\"\"\n",
    "        )\n",
    "    ]\n",
    "    )\n",
    "\n",
    "async def summarize(state: ComplexityState):\n",
    "    chain = summarize_template | model\n",
    "    page_summary = await chain.ainvoke({\n",
    "        \"query\": state['query'], \n",
    "        \"source\": state['content'].metadata['source'], \n",
    "        \"content\": state['content'].page_content})\n",
    "    return {\"page_summaries\":[page_summary.content]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Related questions generation\n",
    "\n",
    "Génère des questions connexes ou \"related questions\" en rapport avec la requête utilisateur. Ce nœud permet d'enrichir le flux en fournissant des pistes d'exploration supplémentaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"\"\"Provide a list of 5 follow up question related to the user one.\"\"\"\n",
    "        ),\n",
    "        ( \"user\", \"{query}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "async def generate_related(state: ComplexityState):\n",
    "    query = state['messages'][0]\n",
    "    chain = related_template | model.with_structured_output(ComplexityRelatedQueries)\n",
    "    result = await chain.ainvoke({\"query\": query.content})\n",
    "    return {\"related_queries\":result.queries}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final answer writer\n",
    "\n",
    "Synthétise les informations obtenues (résumés des pages web, images, vidéos, etc.) pour répondre de manière complète et directe à la question de l'utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are a research writer. Your sole purpose is to write a well-written\n",
    "                research reports about a topic based on research findings and information..\n",
    "\n",
    "                \"\"\",\n",
    "            ),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"\"\"Query or Topic: {query}               \n",
    "                 Your task is to write an in depth, well written and detailed introduction and conclusion to the research report based on the provided research data.\n",
    "                 You MUST include any relevant sources to the introduction and conclusion as markdown hyperlinks -\n",
    "                 \"For example: 'This is a sample text. ([url website](url))'.\n",
    "                 You MUST return nothing but the report. \n",
    "                    ----\n",
    "                Summaries: {page_summaries}\"\"\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "async def writer_answer(state: ComplexityState):\n",
    "    chain = writer_template | model\n",
    "    response = await chain.ainvoke({\n",
    "        \"query\": state['messages'][-1], \n",
    "        \"page_summaries\": state['page_summaries']})\n",
    "    return {\"messages\":[response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer et compiler le graph\n",
    "\n",
    "Dans cette étape, nous créons et compilons le graphe qui orchestre le flux de traitement. Le graphe est instancié avec StateGraph en utilisant la structure d'état ComplexityState, et chaque nœud de traitement y est ajouté pour exécuter les différentes étapes de génération de réponse.\n",
    "\n",
    "Définition des Nœuds : Nous ajoutons chaque fonction créée précédemment comme nœud au graphe de la génération des requêtes utilisateur jusqu’à la création de la réponse finale et des questions complémentaires.\n",
    "\n",
    "Définition des Liaisons :\n",
    "\n",
    "Les étapes de base, comme la recherche web et l'analyse de page, sont reliées de manière linéaire avec add_edge.\n",
    "Exécution parallèle des Résumés : Une fonction _parallel_summarizer est définie pour transmettre chaque résultat scrappé à un nœud de résumé en parallèle, utilisant Send pour fan-out les tâches de résumé à partir des résultats scrappés.\n",
    "Fan-Out : Après le résumé, nous distribuons les tâches vers les nœuds de recherche d'images, de recherche de vidéos et de génération de réponse.\n",
    "Fan-In : Les résultats des recherches multimédias et de la réponse sont ensuite regroupés pour générer des questions complémentaires, complétant ainsi le flux de traitement.\n",
    "Compilation du Graphe : Enfin, nous créons un checkpointer de type MemorySaver pour enregistrer l’état du graphe, puis nous compilons le flux complet avec workflow.compile, produisant une application fonctionnelle qui exécute le workflow.\n",
    "\n",
    "Ce graphe complet orchestre les différentes étapes de recherche, de traitement et de réponse en exploitant des opérations parallèles et des flux de données organisés. Il constitue le cœur du processus de génération de réponse dans notre implémentation de Perplexity AI.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import Send\n",
    "\n",
    "graph = StateGraph(ComplexityState)\n",
    "graph.add_node(\"Génération requête\",genereate_queries)\n",
    "graph.add_node(\"Recherche web\", search_web)\n",
    "graph.add_node(\"Web scrapper\", scrap_web)\n",
    "graph.add_node(\"Recherche images\", search_images)\n",
    "graph.add_node(\"Recherche vidéos\",search_videos)\n",
    "graph.add_node(\"Résumé\",summarize)\n",
    "graph.add_node(\"Générer question complémentaires\",generate_related)\n",
    "graph.add_node(\"Générer réponse\",writer_answer)\n",
    "\n",
    "graph.set_entry_point(\"Génération requête\")\n",
    "graph.add_edge(\"Génération requête\",\"Recherche web\")   \n",
    "graph.add_edge(\"Recherche web\",\"Web scrapper\")    \n",
    "\n",
    "def _parallel_sumarizer(state):\n",
    "    return [Send(\"Résumé\", {\"content\": scrapped_result, \"query\": state['messages'][0].content}) for scrapped_result in state['scrapped_results']]\n",
    "\n",
    "\n",
    "graph.add_conditional_edges(\"Web scrapper\", _parallel_sumarizer,[\"Résumé\"]) \n",
    "\n",
    "#Fan out from summarizer to writer, videos searcher, images searcher\n",
    "graph.add_edge([\"Résumé\"],\"Recherche images\")\n",
    "graph.add_edge([\"Résumé\"],\"Recherche vidéos\")\n",
    "graph.add_edge([\"Résumé\"],\"Générer réponse\")\n",
    "#Fan in back to related questions generator\n",
    "graph.add_edge(\"Recherche images\",\"Générer question complémentaires\")\n",
    "graph.add_edge(\"Générer réponse\",\"Générer question complémentaires\")\n",
    "graph.add_edge(\"Recherche vidéos\",\"Générer question complémentaires\")\n",
    "graph.set_finish_point(\"Générer question complémentaires\")\n",
    " \n",
    "checkpointer = MemorySaver()\n",
    "wokflow = graph.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation du graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(wokflow.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exécution du graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    ")\n",
    "\n",
    "query = \"Is AI in a hype cycle ?\"\n",
    "thread_id=\"05.02-perplexity_clone\"\n",
    "\n",
    "async for chunk in wokflow.astream(\n",
    "    {\"messages\": [HumanMessage(content=query)]},\n",
    "     {\n",
    "        \"callbacks\": [langfuse_handler(thread_id)],\n",
    "        \"configurable\": {\"thread_id\": thread_id}\n",
    "    }, stream_mode=\"updates\",\n",
    "    ):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        print(values)\n",
    "        print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
